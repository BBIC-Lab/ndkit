result_root: 'results'
note: ''

# ==================== dataset parameters
data:
  name: 'NLB'
  session: 'mc_maze'
  data_dir: 'data/nlb_data'

  neural: 
    Delta: 20
    tau_prime: 0

  kin:
    type: 'vel'

  split:
    test_frac: 0.2

# ==================== model parameters
model:
  name: 'DyEnsemble'
  runner_type: 'fit'
  flatten_input: True # whether to flatten the input time dimension
  n_encoding_models: 5
  alpha: 0.9
  n_particals: 200

# ==================== training parameters
train:
  tol_err: 1e5
  err_threshold: 1e-5
  max_epochs: 50
  seed: 3407